-Deep learning can do most tasks that consists of mapping input to output vector which a person can do rapidly given large enough data and model.

# Deep Feedforward Neural Networks 

Feedforward neural networks are function approximators consisting of many hidden layers. Given labelled data $(x, y)$, FNN's goal is to approximate a function $f^\star (x)$ via $f(x; \theta)$ where $\theta$ is learned. The approximation is done via assuming that data is generated by  $y\approx f^\star (x)$. They consists acyclic graph and of many layers so output of a NN is: $y =f^3(f^2(f^1(x)))$. They are feedforward as there is not feedback loop unlike recurrent neural networks. 

> The training examples specify directly what the output layer must do at each point x; it must produce a value that is close to y. The behavior of the other layers is not directly specified by the training data. The learning algorithm must decide how to use those layers to produce the desired output, but the training data do not say what each individual layer should do.   

> It is best to think of feedforward networks as function approximation machines that are designed to achieve statistical generalization, occasionally drawing some insights from what we know about the brain, rather than as models of brain function.  

We can think of DNNs in terms of linear model. A linear models maps input $x$ to output $y$: $y=x^Tw$. However, these models have limited representational power. Instead of using $x$, we can apply a non-linear transform on $x$: $\phi(x)$. We have three options for $\phi$.

1. Use very generic $\phi$: infinite dimensional that is simply used by kernel machines. If $\phi$ has high enough dimensions, we can always fit training data. However, generalization remains poor. 
2. Use handcrafted $\phi$ but it takes a lot of time and effort of humans.
3. Use a strategy to learn $\phi$ and then uses it as follows: $y=\phi(x)^Tw$. We can also encode prior knowledge via regularization. The approach is very generic as we can use families of $\phi$ that we expect to perform good. 

## Example: Learning XOR

Lets assume that we are interested in learning a function $f$ that can represent XOR operation. Also assume that we are only interested in generalization in training part i.e. $X \in {[0,0]^T, [0,1]^T, [1,0]^T ,[1,1]^T }$ and we want to learn $f(x, \theta)$ that can represent this function. Lets use the MSE loss function:

$$J(\theta) = \dfrac{1}{4} \sum_0^3 (y-f(x, \theta))$$

#### Linear Model vs Neural Network 

<img src="imgs/44.png" style="zoom:75%;" />

##### Linear Model

First let's assume the model to be linear:

$f(x, w,b)= x^Tw +b$  

We can minimize cost via a closed form by solving normal equations. With that, we get $w=0$ and $b=\dfrac{1}{2}$. However, this function can not represent a XOR. 

##### Neural Network

Now, let's take a two layer neural network. It can be formulated with following: $h = w_1^Tx+b_1$ and $y=w_2^Th+b_2$ or $f(x, \theta) = f^1(f^2(x))$. However, this is similar to linear model as output can be represented via: $y = w_2^T(w_1^Tx+b_1)+b2 = Wx +b$. We need to add non-linearity to describe the features. In neural nets, we can do this via following: $h = g(w_1^Tx+b_1)$ where g is ReLU which is: $g(z) = \max(0, z)$ and it is applied element wise.

>The function remains very close to linear, however, in the sense that it is a piecewise linear function with two linear pieces. Because rectified linear units are nearly linear, they preserve many of the properties that make linear models easy to optimize with gradient-based methods. They also preserve many of the properties that make linear models generalize well. A common principle throughout computer science is that we can build complicated systems from minimal components. Much as a Turing machine’s memory needs only to be able to store 0 or 1 states, we can build a universal function approximator from rectified linear functions.

The complete network is: $w_2^T. \max(w_1^Tx+b_2)+b_1$.  In this model, we first transform the input into a space where it is linear and then we apply a linear model on that space as shown in the following figure:

<img src="imgs/45.png" style="zoom:75%;" />

## Gradient Based Learning 

We can get exact solutions of linear models, however, this is not possible for neural networks with possibly billions of parameters. For this, we use gradient descent. Gradient descent converges in the case of convex optimization with any initialization (in theory, it may face some numerical problems). For neural networks with interesting loss function, the optimization becomes non-convex and the there is no guarantee for convergence with stochastic gradient descent. However, it works well in practice. It depends on initial values as well. 

Lets review design considerations for neural networks. 

### Cost Functions 

Our parametric model defines distribution $p(y\mid x; \theta)$ and we simply use principle of maximum likelihood i.e. minimize cross entropy between training data and model's prediction. 

Some time we take simple approach where we simply predict a statistic over $y$ instead of entire probability distribution. Specialized cost function enable us to train such predictor. 

The total cost function is always a primary cost function and a regularizer term.

##### Learning Conditional Distributions with Maximum Likelihood

Most modern NNs are trained with maximum likelihood i.e. cross entropy between model distribution and training data:

$$-\mathbb{E}_{x, y \sim \hat{p}_{data}} [\log(p_{model}(y\mid x))]$$

Specific form depends on model to model i.e. on $p_{model}$. Expansion of the cost function often give us cost that depends on model and constant factor.  For instance, if $p_{model}(y\mid x) = \mathcal{N}(y; f(x;\theta, I))$ then we get mean square error:
$$J(\theta) = \dfrac{1}{2} \|y-f(x;\theta)\|^2 + const$$

Discarded portion is based on variation of model which we are not parametrizing.

> Previously, we saw that the equivalence between maximum likelihood estimation with an output distribution and minimization of mean squared error holds for a linear model, but in fact, the equivalence holds regardless of the f(x; θ) used to predict the mean of the Gaussian  

Specifying maximum likelihood means the burden of designing cost function is no more on us for each model. Specifying a model $p(y\mid x)$ mean cost function is $-\log(p(y\mid x))$.

1. One important point in gradient learning is that gradient should be large and predictable enough to guide the learning process. Functions that saturate (becomes flat) does not work well with this objective. Negative log likelihood helps to avoid this issue. For instance, functions involving exp function can saturate but log can undo the exp of some units. 
2. Cross Entropy usually don't have minimum value for commonly used models. For discrete output variables, most models are parametrized in such a way that they can not represent probability of zero or one and only can go very close to it. 

##### Learning Conditional Statistics 

- Sometime we are only interested in a specific conditional statistic $y$ given $x$ instead of full conditional distribution $p(y \mid x)$, i.e. a function to predict mean of $y$.

- A sufficiently powerful neural network can represent any function $f$ from a wide class of functions (limited by some features like continuity and boundedness).  So we can think of neural network as a function (instead of specific set of parameters) and cost function as functional (a functional maps function to real number). This way learning becomes choosing a function. 

- From calculus of variations, we have following two results:

  $$f^* = \arg \min_{f} \mathbb{E}_{x,y \sim p_{data}} \|y - f(x) \|^2 $$
  
  yields 
  
  $$f^* = \mathbb{E}_{y\sim p_{data}(y \mid x)} [y]$$
  
  as long as this function is within class we optimize.  This mean given enough training data, minimizing mean squared cost would give a function that predicts the mean of $y$ for each $x$.
  
  $$f^* = \arg \min_{f} \mathbb{E}_{x,y \sim p_{data}} \|y - f(x) \|_1 $$
  
  yields a function that predict median of $y$ given $x$.
  
- Unfortunately, these two functions often lead to poor results with gradient based learning because some output units saturate and produce very small gradients. That's why cross entropy is famous even when we don't have to estimate entire distribution. 

### Output Units 

**High Level Summary**: We want to produce output from the last layer that resembles desired output e.g. in classification, we want to produce probability distribution. However, we also have to keep in mind cost function and overall gradient. We should choose an output unit that, combined with cost function, should produce a gradient that is appropriate during learning, and should not cause a vanishing or exploding gradients.



- Cost function is coupled with choice of  output unit and choice of output unit decides form of cross entropy function.
- Principally, any output unit can also be used as hidden unit but here we are only interested in output.
- We assume that  neural network has given set of features $h = f(x; \theta)$ and role of output unit is to do additional transformation on the features to complete the task.

##### Linear Units with for Gaussian Output Distributions

- The most simple choice for output unit is linear i.e.. $\hat{y} = W^Th + b$, also called linear units. 

- They are used to produce mean of conditional gaussian: $p(y \mid x) = \mathcal{N}(y; \hat{y}, I)$. Maximizing log likelihood is now minimizing mean squared error. 

- Linear units don't saturate and they pose little difficulty for gradient based learning and can be used with wide variety of optimization algorithms. 

#### Sigmoid Units for Bernoulli Distribution 

- Many tasks require  predicting value of a variable $y$ i.e. binary classification. The maximum likelihood way is to define Bernoulli Distribution over $y$ conditioned on $x$ which is a single number. Neural net needs to predict one number: $p(y=1 \mid x)$, which need to be in the interval [0,1].

##### Why we should use Maximum Likelihood with Sigmoid

- One simple option is to use following function which  converts the output into required range:

  $$f(x) = \max(0, \min(1, w^Tx+b))$$

  However, it has zero gradients all over except $(0,1)$ which means we gradient based learning don't have guide on how to improve corresponding parameters. (red f(x) and black gradient)
  <img src="imgs/48.png" style="zoom:30%;" />

- We want to use an approach that make sure that there is a strong gradient whenever our algorithm gives wrong answer. 

  Let's assume that $\hat{P}(y)$ is unnormalized probability distribution then:

  $$\log \hat{P}(y) = y.z $$

  $$\hat{P}(y) = \exp(yz) $$

  $$P(y) = \dfrac{\exp(yz)}{\sum_{i=0}^1 exp(y^i z)}  $$

  $$P(y) = \sigma((2y-1)z)$$

  z is often called logits. The log in maximum likelihood undo exp of sigmoid. If we use maximum likelihood, we can write cost as:
  $$
  J(\theta) = - \log P(y \mid x) \\
  = - \log (\sigma(2y-1)z)  \\
  = sf((1-2y)z)
  $$
  

  === Correction ===
  
  If we apply negative log likelihood on sigmoid, overall function becomes softplus: $-\log\sigma(x) = -\log\bigg(\dfrac{1}{1+e^{-x}}\bigg) = 0 + \log(1+e^{-x}) = \text{softplus}(x)$, which solves the issue of zero gradient.
  
  $-\log (\dfrac{e^x}{1+e^x}) = -\log e^x - \log(1+e^x) = -x - \log(1+e^x) = -x +\text{softplus(x)}$
  
  Following is the plot of $\color{blue}soft plus$$(\log(1+e^x))$ and its $\color{red}gradient $. 
  
  <img src="imgs/47.png" style="zoom:30%;" />

Gradient of $J(\theta)$ should be positive whenever model has wrong answers and near 0 otherwise.

1. Above-mentioned soft plus has saturated gradient (near zero) whenever answer is negative. The output of softplus is negative whenever both y and z have same signs (i.e. model has right answer) e.g. y=+1, z=+1 ==> (1-2)(1) = -1 or  y=-1, z=-1 ==> (1+2)(-1) = -3.

2. Softplus has $sign(z)$ gradient whenever soft-plus is on positive side. When y and z have different signs (i.e. model is wrong)e.g. $y=+1, z=-1$ means $sf((1-2)(-1)) = +1$ which is in the positive side where gradient is sign(z). This means gradient based learning can act to correct whenever model prediction is wrong.   

On the other hand, mean square loss can saturate anytime $\sigma(z)$ saturates because gradient of sigmoid goes to zero if z is too negative or too positive. $\color{blue} \sigma(z)$ and its $\color{red} gradient$.

 <img src="imgs/46.png" style="zoom:33%;" />

==Correction==

>In software implementations, to avoid numerical problems, it is best to write the negative log-likelihood as a function of z, rather than as a function of $\hat{y} = σ(z)$. If the sigmoid function underflows to zero, then taking the logarithm of $\hat{y}$ yields negative infinity.  



#### Softmax for Multinoulli Output Distribution 

We use softmax when we want to model discrete probability over $n$ possible values. In this case, we want to produce a vector $\hat{y}$ such that $\hat{y}_i = P(y=i, x)$ (between 0 and 1) and sum of $\hat{y}$ is one. First a linear layer gives us un-normalized probability or log probability or logits: $ z = W^Th +b$ where $z_i = \log \hat{P}(y=i \mid x) $.

**The soft max**
$$
\text{softmax}(z_i) = \dfrac{\exp(z_i)}{\sum_{j} \exp(z_j)}
$$
**Softmax after Max-Likelihood Cost Function:** Exponent works well when we use max likelihood because it undo the log of cost function. Consider log the softmax:
$$
- \log \text{ softmax}(z)_i = -(z_i - \log \sum_j \exp(z_j))
$$

- This means that $z_i$ will always have a direct contribution and it will avoid the saturation of log-softmax which in turn will also avoid gradient going to zero even when second term is very small.

- When we maximize log-softmax, we push first term up and push all other terms down. We can approximate second term: $\log \sum_j z_j \approx \max_j z_j$. This means that maximum likelihood will penalize most active term in $\log \sum_j z_j$ 

- If $z_i$ (correct probability) is high than $max_j z_j \approx  z_i$ and this will cancel overall value e.g. $z_i - z_i$, and this *example will contribute small in the overall loss function*. Cost function will be dominated by the examples that are yet incorrectly classified. 

- > Overall, unregularized maximum likelihood will drive the model to learn parameters that drive the softmax to *predict the fraction of counts of each outcome observed in the training set*:  
  > <img src="imgs/49.png" style="zoom:75%;" />

  

##### Why Softmax wouldn't work with other cost functions

If an objective function don't have **log to undo the exp** of SoftMax, it will not perform well because SoftMax saturates when difference between input is extreme which causes the gradient to be vanish and no useful learning can be derived from wrong or right predictions. 
>In particular, squared error is a poor loss function for SoftMax units and can fail to train the model to change its output, even when the model makes highly confident incorrect predictions (Bridle, 1990).  

Observe that adding a scalar value to SoftMax does not have an effect on overall results:

$$softmax(z) = softmax(z + c)$$

using this we can derive a numerically stable version:

$$\text{softmax}(z) = \text{softmax}(z - \max_i z_i)$$

##### Winner-Take-It-All

In SoftMax, increase in one unit's value means decrease in other units as sum is one. This is similar to inhabitation that exits  in nearby neurons in cortex. Similarly, when one $z_i$ is large it becomes winner-take-it all because one value would be close to one and all other values would be close to zero. 

**SoftArgMax**: SoftMax is continuous and differentiable version of argmax and SoftMax is a bit misleading. The continuous version for max would be: $\text{softmax}
(z)^Tz$.

#### Other Output Types

Neural network can generalize to any output layer we may wish to use and principle of maximum log likelihood ($-\log p(y \mid x; \theta)$)  provides a guide  to design a good cost function for any kind of output layer. 

We can think of neural networks as representing $f(x; \theta)$ and output of this is not direct predictions of $y$. Instead $w = f(x; \theta)$ which controls the distribution over $y$ and our loss function can be defined: $-\log p(y; w(x))$. 

While deriving cost function and output layer, we need to keep following in mind:

1. gradients of multiplication , addition and log are well behaved 
2. gradients of division and square are not. Division has arbitrary steep gradients near zero and square have vanishing gradient near zero.
3. While large gradients are good for learning, arbitrarily large gradients usually result in instability. 

###### Multi-Modal Regression 

Predict values from conditional probability distribution $p(y \mid x)$ that can have several peaks in y space for the same value of x.  In this case Gaussian mixture is a good output choice.
$$
p(y \mid x) = \sum_i p(c=i\mid x) \mathcal{N}(y; \mu^i(x), \Sigma^i(x))
$$
A neural network with Gaussian mixture as their output are often called mixture density networks. A neural network should have three outputs:

1. A vector defining $p(c=i \mid x)$:  mixture components over n different components associated with latent variable, SoftMax on output vector
2. A matrix defining $\mu^i(x)$ for all $i$: unconstrained 
3. A tensor defining $\Sigma^i(x)$ for all $i$.

It is effective for generative models of speech and movements of objects. 

## Hidden Units 

- Design of hidden units is an active area of research and there is no definitive theoretical guide on how to choose it, only a few intuitive principles.  **However, many different variants of hidden units functions work equally well. Similarly, a variety of differentiable functions work well.** So, in order to find merit of a new hidden unit, we need to clearly show that it is significantly more useful compare to others.

  >New hidden unit types that perform roughly comparably to known types are so common as to be uninteresting.  

- Choosing a hidden unit depends on application and usually found by trial and error: *intuiting which unit will work well and then training a network with that kind of hidden unit and evaluating its performance on validation set* . However, Rectified Linear Unit (ReLU) is a good default choice. 

- Some hidden units are not differentiable at all inputs but gradient descent still works if it is only non-differentiable at small number of points? For instance, ReLU ($\max(0, z)$) is not differentiable around 0 and yet it works really well. That is because GD based algorithms don't arrive at local minimal but are expected only to minimize the cost function significantly. 

- A function usually have a left derivative (slope to the left of the function) and a right derivative (slope to the right of the function). The function is differentiable at a point $z$ iff both derivates are defined and equal. However, ReLU have 0 left derivative and 1 right derivative at $z=0$. Software implementation usually return one-sided derivative. 

- One important think during the design of activations functions is ease of optimization and if a model's behavior is close to linear, it is easier to optimize. ReLU, for this reason, is a popular choice. 

- A hidden unit usually consists of a affine transformation and an activation function (non-linearity $g(.)$): $g(z) = g(W^Tx + b)$.

### Rectified Linear Unit (ReLU)

ReLU is defined as:
$$
g(z) = \max(0, z)
$$

- It is easy to optimize because it is very similar to linear unit . Its gradient is large whenever it is active and also consistent. 

  The second derivative of the rectifying operation is 0 almost everywhere, and the derivative of the rectifying operation is 1 everywhere that the unit is active. This means that the gradient direction is far
  more useful for learning than it would be with activation functions that introduce second-order effects.  

  This means gradient direction is much more useful compare to the units that introduce second-order effects? 

- It is  typically used with affine transformation: $g(W^Tx+b)$. We should initialize $b$ with small positive values like 0.1 so that unit is initially active for most inputs and allow derivative to pass through. 

#### Non-Zero Slope Based Generalizations 

One issue of ReLU is that it not useful for examples for which activation is zero. Some of its generalizations solve this problem. Theses generalization are based on using a non-zero slope ($\alpha_i$) when $z_i<0$: 
$$
g(z_i) = \max(0, z_i) + \alpha_i \min(0,z_i)
$$

1. **absolute rectification**: $\mid z_i\mid$ is when $\alpha_i=-1$. It is used in object recognition where:

   >it makes sense to seek features that are invariant under a polarity reversal of the input illumination.  

2. **Leaky ReLU**: fixes $\alpha_i$ to small value such as 0.01.
3. **Parametric ReLU (PReLU)**: Learns $\alpha_i$.

#### Max-Out 

In maxout unit, output is divided into $k$ parts and max value is returned in each part: 
$$
g(z)_i = \max_{j \in G^i} z_j
$$
[x x x | x x x | x x x]

Where $G^i$ are set of indices in group $i, \{(i-1)k+1,...ik\}$

This way, we learn a piece wise linear function that responds to multiple directions in the input space $x$.

- A maxout unit can learn a piecewise linear convex function with upto $k$ pieces. Maxout is more like a learning a activation unit. For example, a maxout unit with two pieces can learn ReLU, leaky ReLU, PReLU or any other activation function. However, the leanring dynamics will be different even if it learns exactly ReLU. 

  > A maxout unit can learn a piecewise linear, convex function with up to k pieces. Maxout units can thus be seen as learning the activation function itself rather than just the relationship between units.  

- A maxout has now $k$ weight vectors compare to 1 weight vector in ReLU and require more regularization. Although with large enough data, it can do well without regularization as well if number of pieces are kept low. 

- It may also require less number of parameters and can give statistical and computational benefits. 

  >Specifically, if the features captured by n different linear filters can be summarized without losing information by taking the max over each group of k features, then the next layer can get by with k times fewer weights.  

- It is driven by multiple filters so it can resist **Catastrophic forgetting**.

### Logistic Sigmoid and Hyperbolic Tangent 

Logistic unit:
$$
g(z) = \sigma(z) = \dfrac{1}{1+e^{-z}}
$$
Tangent unit:
$$
g(z) = \tanh(z)
$$
both are closely related as $\tanh(z) = 2\sigma(2z) - 1$.

- **Saturation Issue**: Sigmoidal units saturates to zero (in negative domain) and 1 (positive domain) which can hinder gradient based learning because the gradient signal becomes useless. They are useful as output unit is used with appropriate cost function. 
- **Tanh better than Sigmoid**: Whenever it is required to use them, it is better to use in Tanh because resembles identity near 0 (i.e.$\tanh(0) = 0$ but $\sigma(0)=0.5$) and training NN can be similar to training a linear model as long as activation is kept small.  
- **More Useful in Other Settings**: They are used in recurrent networks and autoencoders because of requirements that rule out linearity. 

### Other Units 

Many different variants of popular activation units as well as many differentiable functions work well.

1. **No Hidden Unit**: Neural network will be linear but it can provide some benefits. Lets assume: $h1 = g(W^Tx+b)$ with $n$ inputs and $p$ outputs and similar thing with two layers: $h2 = g(V^TU^Tx+b)$. $W$ will contain $np$ parameters but $V^TU^T$ will contain $(n+p)q$. For small q, it means considerable saving of parameters. However the linear transformation is of low rank.

2. **SoftMax**: $softmax(z_i) = \dfrac{\exp(z_i)}{\sum_{j} \exp(z_j)}$and it represents discrete probability distribution over $k$ possible values and can be used as a switch. These are used when we need to learn to manipulate memory. 

3. **Radial Basis Function**: 

4. **Soft-Plus**: $g(a) = \log(1+e^a)$. Smooth version of rectifier. 

   >The use of the softplus is generally discouraged. The softplus demonstrates that the performance of hidden unit types can be very counterintuitive—one might expect it to have an advantage over the rectifier due to being differentiable everywhere or due to saturating less completely, but empirically it does not.  

5. **Hard tanh**: $g(a) = max(-1, \min(1, a))$ similar to tanh and rectifier but bounded. 

## Architecture Design 

- **Architecture** of a NN is its overall structure: units in each layer, how to connect these layers and how many layers.
- Most NNs consists of group of units called layers and they are connected with each other in a chain structure i.e. layer 1:  $h^1 = g^1(W^{(1)T}x+b^1)$, layer2: $h^1 = g(W^{(2)T}h^2+b^2)$ etc. In this kind of structure, most important consideration is to find number of layers and number of units per layer.
- Theoretically, it is possible to fit any training set with only one hidden layer with sufficient number of hidden units. Deep networks often require less number of units per layer and generalize well on test set but hard to optimize. 
- **Finding an Architecture**: Practically, architecture for a task must be found via experimentation guided by validation error. 

### Universal Approximation Theorem 

- **Linear Model**:  Simple linear mapping from features from output via matrix multiplication (e.g. $(W_2(W_1x+b_1)+b_2)$) can only represent linear functions. However, they are easy to optimize as many loss will end up with aa convex optimization problem.

- **Learning Non-Linear Function**: To learn a non-linear function, we may expect to use specific kind of non-linearity for that kind of function. However, generic FNNs with hidden layers provide a universal approximation framework. 

  > A FNN with linear output layer and at least one hidden layer with any squashing activation function can approximate any Boreal measurable function (any continuous function on closed and bounded subset of $R^n$) from one finite-dimensional space to another with any desired non-zero error given enough number of units. It can also approximate derivate of the function arbitrarily well. 
  >
  > A NN may also approximate any function from on discrete finite-dimensional space to another.  

- **Practical Implications and Depth**:

  -  A MLP can represent any function i.e. given a function, there exists a FNN that can approximate that function arbitrarily well. However, learning can still fail for two reasons: 1) FNN may approximate wrong function and overfit the training data 2) optimization algorithm may to be able to find the values of the parameters that  correspond to desired output.  So FNN provide a general framework for function approximation, but there is not guarantee that choosing a function will also generalize well to points outside the training set. 

  - Approximation with one hidden layer may require exponential number of units (possibly one hidden unit for each input configuration). Binary example: $v\in \{0, 1\}^n$ means $2^n$ possible configurations and each configuration can results in two possible outputs i.e. $2^{2^n}$ possible functions. Selecting one function require $2^n$ bits and $O(2^n)$ degrees of freedom. 

  - Various families of functions can be approximated well by an architecture with depth greater than equal to some $d$. However, restricting depth $\leq d$ requires much larger model. Shallow model often require exponential number units in input $n$.

  - A theorem states that the number of linear regions carved by deep rectifier network with $d$ inputs, $l$ depth and $n$ units per hidden layer is:
    $$
    O\bigg(\big(n, d)^{d(l-1)} n^d\bigg)
    $$
    i.e. exponential in depth i.e. more layers means exponentially more linear regions.
    <img src="imgs/50.png" style="zoom:50%;" />
    *Increasing number of layers increases accuracy*

- **Implicit Prior Belief**: When we choose an ML model, we are implicitly stating a set of prior beliefs we have about what kind of function it should learn. In DNNs, this is quite general i.e. function to be learned should be composition of several simple functions.

  >This can be interpreted from a representation learning point of view as saying that we believe the learning problem consists of discovering a set of underlying factors of variation that can in turn be described in terms of other, simpler underlying factors of variation. 
  >
  >Alternately, we can interpret the use of a deep architecture as expressing a belief that the function we want to learn is a computer program consisting of multiple steps, where each step makes use of the previous step’s output. 

### Other Choices 

- Many NNs are specifically build for different applications such as CNNs or sequence models
- Layer may not be connected in chains i.e. residual connections where each $i$-th layer is also connected to $i+2$ or higher layer with skip connection for ease in gradient flow. 
- Another choice is how to connect chains of layers. In FNNs, each unit in layer $l$ is connected to each unit in $l+1$. However, we can decreases  number of connections as well.  



