# Chapter 2: Linear Algebra

- **Tensor**: Matrix with variable number of axis.

- **Broadcasting**: Adding a vector to a matrix: $C = \bold{A}+\bold{b}$. Where each column of $\bold{A}$ is added with vector b.

- Multiplication of matrices
  $$
  \bold{C} = \bold{A}\bold{B} = \sum_k A_{ik} B_{kj}
  $$

### Inverse 

- We can write linear equations as $\bold{Ax} = \bold{b}$ where $\bold{x}$ is unknown. We can find solution of this iff $\bold{A^{-1}}$ exists. 

- $$A^{-1}$$ should not be used for practical purposes as digital computers have limited precision. 

- In order for $A^{-1}$ to exits, $x=A^{-1}b$ should have exactly one solution. However, $A^{-1}$ can have no or infinitely many solutions for some values of $b$. It is not possible that $x=A^{-1}b$ have less than infinitely solutions.

- If x and y are solutions of $Ax=b$, then for any real number $a$, following are also the solutions;
  $$
  z = \alpha x + (1-\alpha)y
  $$

  >To analyze how many solutions the equation has, think of the columns of A as specifying different directions we can travel in from the origin (the point specified
  >by the vector of all zeros), then determine how many ways there are of reaching $b$.
  >
  >In this view, each element of $x$ specifies how far we should travel in each of these
  >directions, with $x_i$ specifying how far to move in the direction of column i: $Ax = \sum_i x_i A_{:,i}$  
  
- > Determining whether $Ax = b$ has a solution thus amounts to testing whether b
  > is in the span of the columns of A. This particular span is known as the column
  > space, or the range, of A.  

- $Ax=b$ have a solution for all $b \in R^m$ if **column space of $A$ is $R^m$** or $A$ has m linearly independent columns.
  Example: Assume a 3x2 matrix, the b is 3-D vector. 3x2 matrix is  a slice of 3-D plane so we have exact solution iff b lies on the same 2-D plane. 

- **For solution to exist( column space of A to be m)**: n>=m and all there should be a set of m mutually linearly independent columns (rank(A)>=m). For inversion of A, it should also be square. 

- ![](imgs/3-4.png)

### Norms

- A norm is function mapping of a vector to a non-negative number . It measure size of a vector. A norm is any function that staisfies following three properties 

  1. $f(x)=0 \implies x=0$
  2. $f(x+y) \leq f(x)+f(y)$
  3. $\forall_xf(\alpha x) = |\alpha| f(x) $ 

- An $L_p$ norm is defined as:

  $||x||_p = (\sum_i |x_i|^p)^{\dfrac{1}{p}}$

- **Why square L2 norm?**: Square of $L_2$ norm is easy to work with due to its properties e.g. derivative of  square of l2 norm w.r.t. an element of x is only dependent on that element while for l2 norm it is dependent on the entire vector. 

- Square of l2 norm increases slowly around origin so it may not be useful in some problems. In that cases, L1 norm can be used which increase by e whenever any element of vector x moves by e. 

- Diagonal matrix are useful because of cheap computation. 

- Symmetric matrix (A=A.T) often arise when entries of matrix are generated by a function that don't depend on their order. E.g. if A is distance between point $i$ and $j$ then $A_{i,j} = A_{j,i}$. 

### Decompositions 

- We can decompose matrix to show functional properties that is not obvious from its original representation. 

  >mathematical objects can be understood better by breaking them into constituent parts, or finding some properties of them that are universal, not caused by the way we choose to represent them  
  >
  >we can also decompose matrices in ways that show us information about their functional properties that is not obvious from the representation of the matrix as an array of elements.  

##### Eigen Value Decomposition 

- Let's assume that each column of a matrix $A$  represents a dimension in space $\mathbb{R^m}$ then multiplying it with a vector $x$ i.e. $A.x$ means stretching $x$ in space in all the dimension: $i$-th dimension in space will be stretched by $A_{:,i}$.  Rotation.
  In other words, new vector is 
  
- **Eigen Values, Vectors**: However, Eigen vector is non-zero vector that upon multiplication with $A$ only rescales $v$ : $Av = \lambda.v$ where $\lambda$: eigen value. If $v$ is an eigen vector than any $s.v$ is also eigen vector for all $s\neq0$.

- We can decompose $A=V. \Lambda . V^{-1}$. Where $V = [v_1, v_2, .., v_n ]$  and $\Lambda=daig(\lambda_i)$

  > ..constructing matrices with specific eigenvalues and eigenvectors enables us to stretch space in desired directions.  

- For **real symmetric matrix**: $A = V. \Lambda . V^T$and Every symmetric matrix A has eigen value decomposition.  

	>Because V is an orthogonal matrix, we can think of A as scaling space by $\lambda_i $ in direction $v_i$  
	
- > If any two or more eigenvectors share the same eigenvalue, then any set of orthogonal vectors lying in their span are also eigenvectors with that eigenvalue, and we could equivalently choose a $V$ using those eigenvectors instead.
  >
  > By convention, we usually sort the entries of in descending order. Under this convention, the eigen decomposition is unique only if all the eigenvalues are unique.  

![](imgs/2-1.png)

![](imgs/2-2.png)

##### Singular Value Decomposition

- Every real matrix has an SVD
  $A = U.D.V^T$ where
  - The left-singular vectors of $A$ i.e. columns of $U \in S^{m\times m}$  are the eigenvectors of $AA^T$.
  -  The right-singular vectors of $A$ i.e. columns of $ V^T\in S^{n \times n}$ are the eigenvectors of $A^TA$.
  -  The nonzero singular values of A i.e. digonal entries of $D$ are the square roots of the eigenvalues of  $A^TA$. The same is true for $AA^T$.  

### Moore-Penrose pseudoinverse  

It is solution to following optimization program

$x^* = argmin_x ||b-Ax||_2 = (A^TA)^{-1}Ab$

where first part is the inverse. However, we can also define this pseudo inverse as:

$A^{\dagger} = (A^TA)^{-1}A = U.D^{+} V^{T}$

And $D^{+}$ is inverse of diagonal matrix by reciprocal of non-zero values. 

This solution has three properties

1. Whenever possible, it will give us exact solution
2. If no solution exists then it gives $x$ such that $Ax$ is as close to $b$ as possible in terms of $\|b-Ax\|_2$ 
3. If infinitely many solutions exist, it gives a solution with minimum $||x||_2$

